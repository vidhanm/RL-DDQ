# Self-Improving Debt Collection AI Agent

A reinforcement learning voice agent that learns optimal debt collection strategies using **DDQ (Dyna-style Data-efficient Q-learning)** with LLM integration, adversarial self-play, and multilingual support.

## ğŸš€ Quick Start

```bash
# Install
pip install -r requirements.txt

# Set API key
export OPENAI_API_KEY="your-key"  # or set NVIDIA_API_KEY

# Train (DDQ recommended)
python scripts/train.py --algorithm ddq --episodes 100

# Or DQN baseline
python scripts/train.py --algorithm dqn --episodes 100

# Run web UI
python -m uvicorn web.backend.main:app --reload
# Open http://localhost:8000
```

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| **DDQ Algorithm** | 5-10x faster learning via world model imagination |
| **9 Strategies** | Empathy, plans, settlements, gentle urgency, etc. |
| **Adversarial Training** | Collector vs resistant debtor self-play |
| **Multilingual** | English, Hindi, Hinglish support |
| **Domain Randomization** | Millions of unique debtor profiles |
| **Expert Rewards** | Encodes debt collection best practices |
| **Web Dashboard** | Real-time training visualization |

### ğŸ² How Domain Randomization Works

Instead of training on 4 fixed debtor types, we **randomly generate** personality traits for each conversation:

```
Each debtor = random mix of:
â”œâ”€â”€ Agreeableness (0-100%)     â†’ How cooperative?
â”œâ”€â”€ Emotional Stability (0-100%) â†’ Calm or reactive?
â”œâ”€â”€ Financial Stress (0-100%)    â†’ How desperate?
â””â”€â”€ Life Event (job loss, medical, divorce, none)
```

**Why?** The agent learns to handle *any* debtor, not just 4 scripted ones. Like training a driver on random roads instead of the same 4 routes.


## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Debtor     â”‚â”€â”€â”€â–¶â”‚  NLU State   â”‚â”€â”€â”€â–¶â”‚  DDQ Agent   â”‚
â”‚ (LLM/Adver.) â”‚    â”‚  Extraction  â”‚    â”‚  (Q + World) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â–¼
â”‚   Response   â”‚â—€â”€â”€â”€â”‚  LLM Text    â”‚â—€â”€â”€â”€  Strategy
â”‚   (Voice)    â”‚    â”‚  Generation  â”‚      Selection
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
RL DDQ/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/           # DDQ, DQN, Adversarial agents
â”‚   â”œâ”€â”€ environment/     # NLU env, SelfPlay env
â”‚   â”œâ”€â”€ llm/             # NVIDIA/OpenAI clients, prompts
â”‚   â”œâ”€â”€ nlu/             # State extraction (sentiment, intent)
â”‚   â””â”€â”€ utils/           # Replay buffer, encoders
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_selfplay.py    # Adversarial training
â”‚   â””â”€â”€ evaluate.py          # Evaluation & demo
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ backend/         # FastAPI + WebSocket
â”‚   â””â”€â”€ frontend/        # Dashboard UI
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ RESEARCH_INSIGHTS.md  # 42 papers, 6 topics
â””â”€â”€ data/                # Checkpoints, logs
```

## ğŸ® Action Space (9 Strategies)

1. **Empathetic Listening** - Show understanding
2. **Ask About Situation** - Gather context  
3. **Firm Reminder** - Professional assertive
4. **Offer Payment Plan** - Installments
5. **Propose Settlement** - Reduced amount
6. **Hard Close** - Urgency with consequences
7. **Acknowledge & Redirect** - Handle venting
8. **Validate Then Offer** - Deep empathy â†’ solution
9. **Gentle Urgency** - "Protect your credit score"

## âš”ï¸ Adversarial Self-Play

Train robust collectors against 7 adversary resistance strategies:

| Adversary | Tactic |
|-----------|--------|
| Aggressive | "Stop calling! This is harassment!" |
| Evasive | "Let me think about it..." |
| Emotional | "I can't take this anymore..." |
| Negotiate Hard | "90% off or nothing" |
| Partial Cooperate | Fake interest, no commitment |
| Stall | "Send documents first" |
| Dispute | "Prove this debt is mine" |

```bash
# Full adversarial training
python scripts/train_selfplay.py --generations 20 --episodes 100 --use-llm
```

## ğŸ“Š Web Dashboard

| Page | URL | Features |
|------|-----|----------|
| Home | `/` | Project overview |
| Training | `/train` | Train agents, view metrics |
| Evaluation | `/evaluate` | Test conversations |
| **Adversarial Arena** | `/adversarial` | Live self-play battles |

## ğŸ”¬ Research

Comprehensive research across 6 topics (42 papers):

1. Model-Based RL & Planning
2. Task-Oriented Dialogue RL  
3. Self-Improvement & Meta-Learning
4. Adversarial Training & Robustness
5. Efficient RL / Few-Shot Learning
6. Voice/Spoken Dialogue Systems

**See [docs/RESEARCH_INSIGHTS.md](docs/RESEARCH_INSIGHTS.md) for actionable insights.**

## ğŸ› ï¸ Configuration

Edit `src/config.py`:

```python
# Key settings
STATE_DIM = 12           # NLU features
ACTION_DIM = 9           # Strategies
IMAGINATION_FACTOR = 5   # DDQ imagination multiplier
LANGUAGE = "english"     # or "hindi", "hinglish"
```

## ğŸ“ˆ Development Status

| Phase | Status |
|-------|--------|
| 1-4: Core DDQ | âœ… Complete |
| 5: Evaluation | âœ… Complete |
| 6: Expert Enhancements | âœ… Complete |
| 7: Adversarial Self-Play | âœ… Complete |
| **Research** | âœ… 42 papers reviewed |

## ğŸ“š References

- **DDQ**: Peng et al. "Deep Dyna-Q" (2018)
- **DreamerV3**: Hafner et al. "Mastering Diverse Domains" (2023)
- **Research**: See [RESEARCH_INSIGHTS.md](docs/RESEARCH_INSIGHTS.md)

---

**Last Updated**: December 5, 2025 | **Status**: âœ… Phase 7 Complete
