# Enhancement Roadmap: Self-Improving RL Debt Collection Agent

> **Document Version**: 1.0  
> **Created**: December 4, 2025  
> **Purpose**: Strategic roadmap for improving the RL agent based on expert research and domain knowledge

---

## Table of Contents

1. [Current State Analysis](#current-state-analysis)
2. [Immediate Priorities (Do Now)](#immediate-priorities-do-now)
3. [Future Enhancements (Do Later)](#future-enhancements-do-later)
4. [Expert Knowledge Insights](#expert-knowledge-insights)
5. [Implementation Timeline](#implementation-timeline)

---

## Current State Analysis

### What We Have (Working)

| Component | Status | Description |
|-----------|--------|-------------|
| DDQ Algorithm | âœ… Complete | Dyna-style learning with world model imagination |
| Domain Randomization | âœ… Complete | Millions of unique debtor profiles |
| NLU State Extraction | âœ… Complete | Deterministic behavioral features from text |
| 6 Action Strategies | âœ… Complete | Empathetic, Firm, Payment Plan, Settlement, etc. |
| Web Demo Interface | âœ… Complete | Interactive demo with conversation display |

### What's Missing

1. **Language Support**: Currently English-only, need Hindi for Indian market
2. **Expert Knowledge**: Reward function doesn't encode domain expertise
3. **Action Granularity**: 6 actions may not cover all conversation nuances
4. **Edge Case Handling**: Agent may struggle with rare but important scenarios

---

## Immediate Priorities (Do Now)

> **Principle**: 20% effort â†’ 80% results. Focus on high-impact, low-effort improvements.

### Priority 1: Expert-Knowledge Reward Shaping â­â­â­

**Effort**: 2-3 hours | **Impact**: Very High

Encode expert debt collection strategies directly into the reward function.

#### Positive Rewards (Encourage)

| Behavior | Reward | Reasoning |
|----------|--------|-----------|
| Open with empathy before demands | +2 | Builds trust, expert best practice |
| Acknowledge situation before payment ask | +2 | Shows understanding |
| Offer flexible payment options | +2 | Increases commitment probability |
| De-escalate hostile debtor | +3 | Critical skill, prevents call failure |
| Ask open-ended questions | +1 | Gathers information, shows concern |
| Keep debtor on call | +1/turn | More turns = more opportunity |

#### Negative Rewards (Discourage)

| Behavior | Reward | Reasoning |
|----------|--------|-----------|
| Use hard close before empathy phase | -3 | Premature, damages trust |
| Ignore debtor's stated circumstance | -2 | Expert mistake #1 |
| Repeat failed strategy | -2 | Inflexibility = failure |
| Threatening/aggressive language | -5 | Illegal in many jurisdictions, ineffective |
| Match hostility with hostility | -3 | Escalates situation |

#### Implementation Location
- File: `src/environment/nlu_env.py` â†’ `_calculate_reward()` method

---

### Priority 2: Hindi Language Support â­â­â­

**Effort**: 1-2 hours | **Impact**: High (Direct business value for India)

The RL logic remains unchanged. Only update:

1. **Agent Prompt Templates** (`src/llm/prompts.py`)
   - Translate opening greetings
   - Translate strategy-specific utterances
   - Keep code-switching support (Hindi + English mix common in India)

2. **Debtor Simulation Prompts** (`src/environment/domain_randomizer.py`)
   - Debtor should respond in Hindi
   - Include cultural context (festivals, family structure, etc.)

#### Example Translations

```
English: "Hello, thank you for taking time to speak with me today."
Hindi: "à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤œ à¤®à¥à¤à¤¸à¥‡ à¤¬à¤¾à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤®à¤¯ à¤¨à¤¿à¤•à¤¾à¤²à¤¨à¥‡ à¤•à¤¾ à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦à¥¤"

English: "I understand discussing debts can be stressful."
Hindi: "à¤®à¥ˆà¤‚ à¤¸à¤®à¤à¤¤à¤¾/à¤¸à¤®à¤à¤¤à¥€ à¤¹à¥‚à¤‚ à¤•à¤¿ à¤•à¤°à¥à¤œ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¬à¤¾à¤¤ à¤•à¤°à¤¨à¤¾ à¤¤à¤¨à¤¾à¤µà¤ªà¥‚à¤°à¥à¤£ à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤"
```

#### Considerations
- Support "Hinglish" (Hindi-English mix): "EMI", "payment", "account" are commonly used
- Use respectful "à¤†à¤ª" form throughout (not informal "à¤¤à¥à¤®")
- Consider regional greetings based on time (à¤¶à¥à¤­ à¤ªà¥à¤°à¤­à¤¾à¤¤/à¤¨à¤®à¤¸à¥à¤•à¤¾à¤°)

---

### Priority 3: Add More Action Strategies â­â­

**Effort**: 1-2 hours | **Impact**: Medium-High

Current 6 actions may not cover all conversation nuances. Add 2-3 more:

#### New Actions to Add

| Action | When to Use | Description |
|--------|-------------|-------------|
| `ACKNOWLEDGE_AND_REDIRECT` | Debtor goes off-topic or vents | "I hear you. Let me see how we can help with that..." then redirect to payment |
| `VALIDATE_THEN_OFFER` | Debtor expresses strong emotion | Acknowledge emotion fully, pause, then gently offer solution |
| `GENTLE_URGENCY` | Need to create motivation | Create urgency without threats: "Acting now protects your credit score" |
| `PROBE_DEEPER` | Debtor gives vague answers | Ask clarifying questions to understand real situation |

#### Implementation Location
- File: `src/config.py` â†’ Add to action definitions
- File: `src/llm/prompts.py` â†’ Add strategy templates

---

## Future Enhancements (Do Later)

> These are powerful but require significant effort. Document for v2.

### Enhancement 1: Adversarial Self-Play Training ðŸ”®

**Effort**: High (2-3 weeks) | **Impact**: Very High

Train two agents competing against each other:
- **Collector Agent**: Learns to get payment commitment
- **Adversarial Debtor Agent**: Learns to resist, find weaknesses

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SELF-PLAY TRAINING                    â”‚
â”‚                                                         â”‚
â”‚  Collector v1 â—„â”€â”€playsâ”€â”€â–º Adversary v1                  â”‚
â”‚       â”‚                        â”‚                        â”‚
â”‚       â–¼ improves               â–¼ improves               â”‚
â”‚  Collector v2 â—„â”€â”€playsâ”€â”€â–º Adversary v2                  â”‚
â”‚       â”‚                        â”‚                        â”‚
â”‚       â–¼                        â–¼                        â”‚
â”‚     ...both keep improving...                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits**:
- Automatically discovers edge cases
- Agent becomes robust against any strategy
- No need to manually design difficult scenarios

---

### Enhancement 2: Meta-Learning (Learning to Learn) ðŸ”®

**Effort**: Very High (Research-level) | **Impact**: Very High

Instead of one fixed policy, agent learns to **quickly adapt** during conversation.

```
Turn 1-2: Exploration (figure out debtor type)
Turn 3+: Exploitation (use adapted strategy)
```

**Technical**: Model-Agnostic Meta-Learning (MAML) or contextual bandits.

**Benefit**: Handles completely novel debtor personalities never seen in training.

---

### Enhancement 3: Multi-Step Planning with World Model ðŸ”®

**Effort**: Medium-High | **Impact**: High

Use existing DDQ world model for look-ahead planning:
- Current: Pick best action for THIS turn (greedy)
- Enhanced: Simulate 3-5 turns ahead, pick action with best trajectory

**Benefit**: Some situations require patience. Short-term bad action may lead to long-term success.

---

### Enhancement 4: Conversation Phase Detection ðŸ”®

**Effort**: Medium | **Impact**: Medium-High

Explicitly model conversation phases:

| Phase | Goal | Appropriate Actions |
|-------|------|---------------------|
| Phase 1: Connection | Build rapport | Empathy, listening |
| Phase 2: Discovery | Understand situation | Open questions |
| Phase 3: Solution | Present options | Payment plan, settlement |
| Phase 4: Commitment | Get agreement | Gentle close, confirm |
| Phase 5: Wrap-up | Ensure next steps | Summarize, schedule |

**Implementation**: Add phase to state representation, train agent to progress through phases appropriately.

---

### Enhancement 5: Explainable Decisions ðŸ”®

**Effort**: Medium | **Impact**: Medium

Add ability for agent to explain why it chose an action:
- "I chose empathy because the debtor's sentiment was negative (-0.6) and they mentioned job loss."

**Benefit**: Compliance, debugging, human oversight.

---

### Enhancement 6: Continuous Online Learning ðŸ”®

**Effort**: High | **Impact**: Very High

Agent keeps learning from real production conversations:
1. Agent deployed
2. Has real conversation
3. Gets feedback (payment happened or not)
4. Updates policy (with safety constraints)

**Challenge**: Must prevent agent from learning bad behaviors.

---

## Expert Knowledge Insights

### What Makes Expert Collectors Different from Beginners

Based on industry research:

| Beginner Behavior | Expert Behavior |
|-------------------|-----------------|
| Aggressive, confrontational | Assertive but empathetic |
| Rushes through calls | Active listening, lets debtor talk |
| Gives up after first rejection | Understands "no" is often temporary |
| One-size-fits-all script | Adapts strategy to individual |
| Focuses only on payment | Focuses on relationship first |
| Gets emotional in difficult calls | Maintains composure always |
| Demands immediate payment | Offers flexible solutions |

### Why Debt Collection Calls Fail

| Failure Category | Specific Mistakes |
|-----------------|-------------------|
| Legal/Ethical | Harassment, threats, wrong call times, lying |
| Communication | Demanding language, ultimatums, inflexibility |
| Preparation | No debtor history research, no documentation |
| Strategy | Delayed action, inconsistent follow-up, rigid scripts |
| Empathy | Ignoring circumstances, not offering alternatives |

### Psychology of Debt Collection

Key cognitive biases to leverage:

1. **Present Bias**: People discount future consequences heavily
   - *Use*: Create immediate importance without threats

2. **Loss Aversion**: Fear loss more than value gain
   - *Use*: Frame as "protect your credit score" not "pay debt"

3. **Reciprocity**: People return favors
   - *Use*: Show understanding first, they'll be more cooperative

4. **Social Proof**: People follow others' actions
   - *Use*: "Most people in your situation find payment plans helpful"

---

## Implementation Timeline

### Phase 1: Quick Wins (Week 1)
- [ ] Implement expert reward shaping
- [ ] Add Hindi prompt templates
- [ ] Add 2-3 new action strategies
- [ ] Update documentation

### Phase 2: Refinement (Week 2-3)
- [ ] Test and tune reward weights
- [ ] A/B test Hindi vs English conversations
- [ ] Collect edge case examples from testing

### Phase 3: Advanced Features (Future)
- [ ] Explore adversarial self-play
- [ ] Add conversation phase detection
- [ ] Consider meta-learning architecture

---

## References

- Industry research on debt collection best practices
- HuggingFace documentation on self-play RL
- Unity ML-Agents adversarial training guide
- Expert negotiation techniques from collection industry guides

---

> **Note**: This roadmap was created during brainstorming session on December 4, 2025. 
> Update as implementation progresses.
